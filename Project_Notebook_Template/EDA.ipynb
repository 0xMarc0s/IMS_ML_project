{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad09b7f8",
   "metadata": {},
   "source": [
    "<b><font size=\"6\">Predictive Modelling Pipeline Template</font></b><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253d4d8",
   "metadata": {},
   "source": [
    "In this notebook we present to you the main steps you should follow throughout your project.\n",
    "\n",
    "\n",
    "<b> Important: The numbered sections and subsections are merely indicative of some of the steps you should pay attention to in your project. <br>You are not required to strictly follow this order or to execute everything in separate cells.</b>\n",
    "    \n",
    "<img src=\"image/process_ML.png\" style=\"height:70px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51dd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.chdir('C:/Users/User/Downloads/Desktop/Machine Learning 1/GroupProject - ML1 (2425)/GroupProject - ML1 (2425)') #cambia directory del kernel\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18852571",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"\">\n",
    "\n",
    "# 1. Import data (Data Integration)\n",
    "\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaea6ca",
   "metadata": {},
   "source": [
    "<img src=\"image/step1.png\" style=\"height:60px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c451a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/obesity_train.csv')\n",
    "test_data = pd.read_csv('data/obesity_test.csv')\n",
    "\n",
    "\n",
    "# Usa display per mostrare il DataFrame intero senza troncamenti\n",
    "display(train_data)\n",
    "\n",
    "train_data.obese_level.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4364942",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"\">\n",
    "\n",
    "# 2. Explore data (Data access, exploration and understanding)\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f42fb",
   "metadata": {},
   "source": [
    "<img src=\"image/step2.png\" style=\"height:60px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e4c1c",
   "metadata": {},
   "source": [
    "Remember, this step is very important as it is at this stage that you will really look into the data that you have. Generally speaking, if you do well at this stage, the following stages will be very smooth.\n",
    "\n",
    "Moreover, you should also take the time to find meaningful patterns on the data: what interesting relationships can be found between the variables and how can that knowledge be inform your future decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of the 'marital_status' column as it doesn't contain useful data\n",
    "train_data.drop(columns=['marrital_status'], inplace=True)\n",
    "\n",
    "# Handling missing values in 'physical_activity_perweek' as 0 workouts\n",
    "train_data['physical_activity_perweek'].fillna(0, inplace=True)\n",
    "\n",
    "# Descriptive statistics for numerical variables\n",
    "numerical_columns = ['age', 'height', 'weight', 'meals_perday', 'siblings']\n",
    "print(\"Descriptive statistics for numerical variables:\")\n",
    "print(train_data[numerical_columns].describe())\n",
    "\n",
    "# Histograms for numerical variables\n",
    "train_data[numerical_columns].hist(bins=20, figsize=(10, 8))\n",
    "plt.show()\n",
    "\n",
    "# Separate boxplots for each numerical variable\n",
    "for column in numerical_columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(data=train_data[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "    plt.show()\n",
    "\n",
    "# Logarithmic transformation for variables with skewed distribution (optional)\n",
    "log_transform_columns = ['weight', 'age']  # Example of variables to transform\n",
    "\n",
    "for column in log_transform_columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(np.log1p(train_data[column]), bins=20, kde=True)\n",
    "    plt.title(f'Logarithmic distribution of {column}')\n",
    "    plt.show()\n",
    "\n",
    "# Ordering of categories for categorical variables\n",
    "alcohol_order = ['Never', 'Sometimes', 'Frequently', 'Always']\n",
    "devices_order = ['up to 2', 'up to 5', 'more than 5']\n",
    "eat_between_meals_order = ['Never', 'Sometimes', 'Frequently', 'Always']\n",
    "physical_activity_order = [0, '1 to 2', '3 to 4', '5 or more']\n",
    "water_daily_order = ['less than 1', '1 to 2', 'more than 2']\n",
    "veggies_order = ['Never', 'Sometimes', 'Always']\n",
    "transportation_order = ['Walk', 'Bicycle', 'Motorbike', 'Car', 'Public']\n",
    "\n",
    "# Countplot for categorical variables (ordered)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='alcohol_freq', data=train_data, order=alcohol_order)\n",
    "plt.title('Distribution of alcohol_freq')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='devices_perday', data=train_data, order=devices_order)\n",
    "plt.title('Distribution of devices_perday')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='eat_between_meals', data=train_data, order=eat_between_meals_order)\n",
    "plt.title('Distribution of eat_between_meals')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='physical_activity_perweek', data=train_data, order=physical_activity_order)\n",
    "plt.title('Distribution of physical_activity_perweek')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='water_daily', data=train_data, order=water_daily_order)\n",
    "plt.title('Distribution of water_daily')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='veggies_freq', data=train_data, order=veggies_order)\n",
    "plt.title('Distribution of veggies_freq')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='transportation', data=train_data, order=transportation_order)\n",
    "plt.title('Distribution of transportation')\n",
    "plt.show()\n",
    "\n",
    "# Statistics for boolean variables\n",
    "boolean_columns = ['gender', 'monitor_calories', 'parent_overweight', 'smoke']\n",
    "\n",
    "print(\"Distribution of boolean variables:\")\n",
    "for column in boolean_columns:\n",
    "    print(train_data[column].value_counts())\n",
    "    print()\n",
    "\n",
    "# Countplot for boolean variables\n",
    "for column in boolean_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x=column, data=train_data)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62171731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Function to perform chi-squared test between categorical variables\n",
    "def chi_square_test(cat_var1, cat_var2):\n",
    "    contingency_table = pd.crosstab(train_data[cat_var1], train_data[cat_var2])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    print(f\"Chi-squared test between {cat_var1} and {cat_var2}:\")\n",
    "    print(f\"Chi-squared statistic = {chi2:.2f}, p-value = {p:.4f}\\n\")\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_columns = ['alcohol_freq', 'devices_perday', 'eat_between_meals', \n",
    "                       'physical_activity_perweek', 'water_daily', 'veggies_freq', \n",
    "                       'transportation']\n",
    "\n",
    "# Perform chi-squared test between all pairs of categorical variables\n",
    "for i in range(len(categorical_columns)):\n",
    "    for j in range(i+1, len(categorical_columns)):\n",
    "        chi_square_test(categorical_columns[i], categorical_columns[j])\n",
    "\n",
    "# Function to perform chi-squared test between categorical variables and the target variable\n",
    "def chi_square_test(cat_var, target_var):\n",
    "    contingency_table = pd.crosstab(train_data[cat_var], train_data[target_var])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    print(f\"Chi-squared test between {cat_var} and {target_var}:\")\n",
    "    print(f\"Chi-squared statistic = {chi2:.2f}, p-value = {p:.4f}\\n\")\n",
    "\n",
    "# Performing chi-squared test between categorical variables and the target variable\n",
    "categorical_columns = ['alcohol_freq', 'devices_perday', 'eat_between_meals', 'physical_activity_perweek',\n",
    "                       'water_daily', 'veggies_freq', 'transportation']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    chi_square_test(column, 'obese_level')\n",
    "\n",
    "# Function to calculate Cramér's V for categorical variables\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(k, r) - 1)))\n",
    "\n",
    "# Correlation for categorical variables\n",
    "categorical_columns = ['alcohol_freq', 'devices_perday', 'eat_between_meals', 'physical_activity_perweek',\n",
    "                       'water_daily', 'veggies_freq', 'transportation']\n",
    "target_variable = 'obese_level'  # The name of the target variable\n",
    "\n",
    "# Calculate Cramér's V for each categorical variable relative to the target variable\n",
    "for column in categorical_columns:\n",
    "    cramers_v_value = cramers_v(train_data[column], train_data[target_variable])\n",
    "    print(f\"Cramér's V between {column} and {target_variable}: {cramers_v_value:.2f}\")\n",
    "\n",
    "# ANOVA analysis for numerical variables relative to the target variable\n",
    "numerical_columns = ['age', 'height', 'weight', 'meals_perday', 'siblings']\n",
    "\n",
    "for column in numerical_columns:\n",
    "    groups = [train_data[train_data[target_variable] == category][column].dropna() \n",
    "              for category in train_data[target_variable].unique()]\n",
    "    anova_result = f_oneway(*groups)\n",
    "    print(f\"ANOVA for {column} relative to {target_variable}: F-statistic={anova_result.statistic:.2f}, p-value={anova_result.pvalue:.4f}\")\n",
    "\n",
    "# Heatmap of correlation for numerical variables\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(train_data[numerical_columns].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix for Numerical Variables')\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix for categorical variables using Cramér's V\n",
    "categorical_corr = pd.DataFrame(index=categorical_columns, columns=[target_variable])\n",
    "\n",
    "for column in categorical_columns:\n",
    "    categorical_corr.loc[column, target_variable] = cramers_v(train_data[column], train_data[target_variable])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(categorical_corr.astype(float), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Cramér's V Correlation Matrix for Categorical Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f809d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order of categories for the target variable 'obese_level'\n",
    "target_order = ['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II', \n",
    "                'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III']\n",
    "\n",
    "# Bar plot for 'eat_between_meals' relative to 'obese_level'\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.countplot(x='obese_level', hue='eat_between_meals', data=train_data, order=target_order)\n",
    "plt.title('Distribution of eat_between_meals relative to obese_level')\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for 'veggies_freq' relative to 'obese_level'\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.countplot(x='obese_level', hue='veggies_freq', data=train_data, order=target_order)\n",
    "plt.title('Distribution of veggies_freq relative to obese_level')\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for 'alcohol_freq' relative to 'obese_level'\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.countplot(x='obese_level', hue='alcohol_freq', data=train_data, order=target_order)\n",
    "plt.title('Distribution of alcohol_freq relative to obese_level')\n",
    "plt.show()\n",
    "\n",
    "# Create a new combined column with 'alcohol_freq' and 'eat_between_meals'\n",
    "train_data['alcohol_eat_combined'] = train_data['alcohol_freq'] + ' & ' + train_data['eat_between_meals']\n",
    "\n",
    "# Order categories for the target variable\n",
    "target_order = ['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II', \n",
    "                'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III'] \n",
    "\n",
    "# Create a new combined column with 'eat_between_meals' and 'veggies_freq'\n",
    "train_data['eat_veggie_combined'] = train_data['eat_between_meals'] + ' & ' + train_data['veggies_freq']\n",
    "\n",
    "# Stacked bar plot for the combination of 'eat_between_meals' and 'veggies_freq'\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='obese_level', hue='eat_veggie_combined', data=train_data, order=target_order)\n",
    "plt.title('Combined distribution of eat_between_meals and veggies_freq relative to obese_level')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Create a new combined column with 'veggies_freq' and 'alcohol_freq'\n",
    "train_data['veggie_alcohol_combined'] = train_data['veggies_freq'] + ' & ' + train_data['alcohol_freq']\n",
    "\n",
    "# Stacked bar plot for the combination of 'veggies_freq' and 'alcohol_freq'\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='obese_level', hue='veggie_alcohol_combined', data=train_data, order=target_order)\n",
    "plt.title('Combined distribution of veggies_freq and alcohol_freq relative to obese_level')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical variables\n",
    "numerical_columns = ['age', 'meals_perday', 'weight', 'height']\n",
    "\n",
    "# Boxplot for each numerical variable relative to 'obese_level'\n",
    "for column in numerical_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x='obese_level', y=column, data=train_data, order=target_order)\n",
    "    plt.title(f'Boxplot between {column} and obese_level')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Function to perform ANOVA test\n",
    "def anova_test(numerical_var, target_var='obese_level'):\n",
    "    model = ols(f'{numerical_var} ~ C({target_var})', data=train_data).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    print(f\"ANOVA for {numerical_var} relative to {target_var}:\")\n",
    "    print(anova_table)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Perform ANOVA test for numerical variables\n",
    "for column in numerical_columns:\n",
    "    anova_test(column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ef91b",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"\">\n",
    "\n",
    "# 3. Modify data (Data preparation)\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b92833",
   "metadata": {},
   "source": [
    "<img src=\"image/step3.png\" style=\"height:60px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3fb00",
   "metadata": {},
   "source": [
    "Use this section to apply transformations to your dataset.\n",
    "\n",
    "Remember that your decisions at this step should be exclusively informed by your **training data**. While you will need to split your data between training and validation, how that split will be made and how to apply the approppriate transformations will depend on the type of model assessment solution you select for your project (each has its own set of advantages and disadvantages that you need to consider). **Please find a list of possible methods for model assessment below**: \n",
    "\n",
    "1. **Holdout method**\n",
    "2. **Repeated Holdout method**\n",
    "3. **Cross-Validation**\n",
    "\n",
    "__Note:__ Instead of creating different sections for the treatment of training and validation data, you can make the transformations in the same cell. There is no need to create a specific section for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5699be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a641d2b",
   "metadata": {},
   "source": [
    "### 3.1. Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c06e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ecf700",
   "metadata": {},
   "source": [
    "### 3.2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef23dfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97a4de74",
   "metadata": {},
   "source": [
    "### 3.3. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09e725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee111e6e",
   "metadata": {},
   "source": [
    "### 3.4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221dfea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14d1e3a1",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"\">\n",
    "\n",
    "# 4 & 5. Model & Assess (Modelling and Assessment)\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad01ff7",
   "metadata": {},
   "source": [
    "<img src=\"image/step4.png\" style=\"height:60px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89849818",
   "metadata": {},
   "source": [
    "### 4.1. Model Selection\n",
    "\n",
    "In this section you should take the time to train different predictive algorithms with the data that got to this stage and **use the approppriate model assessment metrics to decide which model you think is the best to address your problem**.\n",
    "\n",
    "**You are expected to present on your report the model performances of the different algorithms that you tested and discuss what informed your choice for a specific algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92285396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f08f9c24",
   "metadata": {},
   "source": [
    "### 4.2. Model Optimization\n",
    "\n",
    "After selecting the best algorithm (set of algorithms), you can try to optimize the performance of your model by fiddling with the algorithms' hyper-parameters and select the options that result on the best overall performance.\n",
    "\n",
    "Possible ways of doing this can be through:\n",
    "1. [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "2. [RandomSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "**While you are not required to show the results of all combinations of hyperparameters that you tried, you should at least discuss the what were the possible combinations used and which of them resulted in your best performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387961a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebfaed02",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"\">\n",
    "\n",
    "# 5. Deploy\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24677b",
   "metadata": {},
   "source": [
    "<img src=\"image/step5.png\" style=\"height:60px\">\n",
    "\n",
    "### 5.0 Training a final model\n",
    "\n",
    "You used the previous steps of modelling and assessment to determine what would be best strategies when it comes to preprocessing, scaling, feature selection, algorithm and hyper-parameters you could find. \n",
    "\n",
    "**By this stage, all of those choices were already made**. For that reason, a split between training and validation is no longer necessary. **A good practice** would be to take the initial data and train a final model with all of the labeled data that you have available.\n",
    "\n",
    "**Everything is figured by this stage**, so, on a first level all you need to do is replicate the exact preprocessing, scaling and feature selection decisions you made before.<br>\n",
    "When it comes to the final model, all you have to do is creeate a new instance of your best algorithm with the best parameters that you uncovered (no need to try all algorithms and hyper-parameters again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ebbfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f38b4acc",
   "metadata": {},
   "source": [
    "### 5.1. Import and Transform your test data\n",
    "\n",
    "Remember, the test data does not have the `outcome` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf433a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30624f3a",
   "metadata": {},
   "source": [
    "### 5.2. Obtain Predictions on the test data from your final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdf5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27d4b2b2",
   "metadata": {},
   "source": [
    "### 5.3. Create a Dataframe containing the index of each row and its intended prediction and export it to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c3404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95e385c2",
   "metadata": {},
   "source": [
    "Submit the csv file to Kaggle to obtain the model performance of your model on the test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NOVAIMSML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
